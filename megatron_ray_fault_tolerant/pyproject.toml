[project]
name = "ray-ft"
version = "0.0.1"
description = "ray"
authors = [
    {name = "ray", email = "ray@gmail.com"}
]
license = {text = "MIT"}
readme = "README.md"
requires-python = "==3.12.*"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]

dependencies = [
    "ninja",
    "tensorboard",
    "func_timeout",
    "transformers>=4.51.0",
    "torchdata",
    "omegaconf",
    "ray==2.51.0",
    "peft",
    "debugpy==1.8.0",
    "hf_transfer",
    "wandb",
    "datasets==4.0.0",
    "flash-attn",
    "polars",
    "loguru",
    "jaxtyping",
    "s3fs",
    "transformer-engine[pytorch]==2.7.0",
    "flash-attn==2.7.4.post1",
    "vllm==0.10.1.1",
    "torch==2.7.1",
    "flashinfer-python",
    "torchvision",
    "megatron-bridge==0.1.0rc4",
    "megatron-core==0.14.0",
]

[tool.uv]
required-version = ">=0.8.10"
no-build-isolation-package = [
    "transformer-engine-torch",
    "transformer-engine",
]

[tool.uv.extra-build-dependencies]
flash-attn = [{requirement = "torch", match-runtime = true}]
transformer-engine = [{ requirement = "torch", match-runtime = true }, "build_tools"]
transformer-engine-torch = [{ requirement = "torch", match-runtime = true }, "build_tools"]

[tool.uv.extra-build-variables]
flash-attn = { FLASH_ATTENTION_SKIP_CUDA_BUILD = "TRUE"}

[tool.uv.sources]
torch = { index = "pytorch-cu128" }
torchvision = { index = "pytorch-cu128" }
# We use `flashinfer-jit-cache` to avoid slow JIT compilation on first run.
# Different inference engines may pin different compatible flashinfer versions, so we provide the option to pin different versions for vllm/sglang
flashinfer-jit-cache = { index = "flashinfer-cu128", marker = "extra == 'vllm'" }
flashinfer-python = [
    { url = "https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl", marker = "extra == 'mcore' and extra != 'vllm'" },
    { url = "https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl", marker = "extra == 'sglang' and extra != 'mcore' and extra != 'vllm'" }
]

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

[[tool.uv.index]]
name = "flashinfer-cu128"
url = "https://flashinfer.ai/whl/cu128"
explicit = true

[tool.setuptools]
include-package-data = true

[tool.pytest.ini_options]
addopts = "-v -s"
testpaths = [
    "tests",
]