[project]
name = "ray-ft"
version = "0.0.1"
description = "ray"
authors = [
    {name = "ray", email = "ray@gmail.com"}
]
license = {text = "MIT"}
readme = "README.md"
requires-python = "==3.12.*"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]

dependencies = [
    "ninja",
    "tensorboard",
    "func_timeout",
    "transformers>=4.51.0",
    "torchdata",
    "omegaconf",
    "ray==2.51.0",
    "peft",
    "debugpy==1.8.0",
    "hf_transfer",
    "wandb",
    "datasets==4.0.0",
    "flash-attn",
    "polars",
    "loguru",
    "jaxtyping",
    "s3fs",
    # Make sure to change the flash attention source (under tool.uv.sources) above to a compatible version (<= 2.7.4.post1) for TransformerEngine==2.5.0
    # https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.7cxx11abiFALSE-cp312-cp312-linux_x86_64.whl
    # For single node: build transformer-engine separately first, and uncomment the transformer-engine library import below
    # uv pip install "torch==2.7.1"
    # uv pip install "nvidia-cudnn-cu12>=9.3"
    # export CUDNN_PATH="$(python -c 'import inspect, nvidia.cudnn as c, os; print(os.path.dirname(inspect.getfile(c)))')"
    # export CPATH="$CUDNN_PATH/include:${CPATH:-}"
    # export LD_LIBRARY_PATH="$CUDNN_PATH/lib:${LD_LIBRARY_PATH:-}"
    # uv pip install --no-build-isolation "transformer_engine[pytorch]==2.5.0" --verbose
    # "transformer-engine[pytorch]==2.5.0",
    "transformer-engine[pytorch]==2.7.0",
    "flash-attn==2.7.4.post1",
    "vllm==0.10.1.1",
    "torch==2.7.1",
    "flashinfer-python",
    "torchvision",
    "megatron-bridge==0.1.0rc4",
    "megatron-core==0.14.0",
]

[tool.uv]
required-version = ">=0.8.10"
no-build-isolation-package = [
    "transformer-engine-torch",
    "transformer-engine",
]

[tool.uv.extra-build-dependencies]
flash-attn = [{requirement = "torch", match-runtime = true}]
transformer-engine = [{ requirement = "torch", match-runtime = true }, "build_tools"]
transformer-engine-torch = [{ requirement = "torch", match-runtime = true }, "build_tools"]

[tool.uv.extra-build-variables]
flash-attn = { FLASH_ATTENTION_SKIP_CUDA_BUILD = "TRUE"}

[tool.uv.sources]
torch = { index = "pytorch-cu128" }
torchvision = { index = "pytorch-cu128" }
# We use `flashinfer-jit-cache` to avoid slow JIT compilation on first run.
# Different inference engines may pin different compatible flashinfer versions, so we provide the option to pin different versions for vllm/sglang
flashinfer-jit-cache = { index = "flashinfer-cu128", marker = "extra == 'vllm'" }
flashinfer-python = [
    { url = "https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl", marker = "extra == 'mcore' and extra != 'vllm'" },
    { url = "https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl", marker = "extra == 'sglang' and extra != 'mcore' and extra != 'vllm'" }
]

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

[[tool.uv.index]]
name = "flashinfer-cu128"
url = "https://flashinfer.ai/whl/cu128"
explicit = true

[tool.setuptools]
include-package-data = true

[tool.pytest.ini_options]
addopts = "-v -s"
testpaths = [
    "tests",
]