# Anyscale Job configuration for Megatron-Bridge training
# 8 GPUs: 2 worker nodes with 4 GPUs each (g6e.12xlarge)

name: ray-train-megatron-bridge-8gpu-job

# Build a custom image using the local Dockerfile
containerfile: ./Dockerfile  
cloud: mithril

# When empty, Anyscale will auto-select the instance types. You can also specify
# minimum and maximum resources.
compute_config:
# compute_config:
#   head_node:
#     instance_type: m5.xlarge
  worker_nodes:
    - instance_type: 144CPU-1600GB-8xH100 
      min_nodes: 1
      max_nodes: 1

working_dir: .

env_vars:
  RAY_TRAIN_V2_ENABLED: "1"
  CUDA_DEVICE_MAX_CONNECTIONS: "1"  # Required for sequence parallelism
  MEGATRON_BRIDGE_ROOT: "/app/Megatron-Bridge"
  PYTHONPATH: "/app/Megatron-Bridge/src:/app/Megatron-Bridge/3rdparty/Megatron-LM"
  NCCL_DEBUG: "WARN"
  PYTHONUNBUFFERED: "1"

# Simplified entrypoint - TP=2, PP=2, batch sizes are hardcoded in script
entrypoint: |
  python llm_sft_ray_train_megatron.py \
    --hf_model_path Qwen/Qwen2.5-0.5B \
    --num_workers 8 \
    --train_iters 100 \
    --storage_path /mnt/cluster_storage/megatron_experiment
