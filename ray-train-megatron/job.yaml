# Anyscale Job configuration for Megatron-Bridge training
# 8 GPUs: 2 worker nodes with 4 GPUs each (g6e.12xlarge)

name: ray-train-megatron-bridge-8gpu-job

# Build a custom image using the local Dockerfile
containerfile: ./Dockerfile  

# Alternatively, use a pre-built image (ask an Anyscale engineer for access)
#image_uri: anyscale/image/megatron-bridge-ray-train:1 

# When empty, Anyscale will auto-select the instance types. You can also specify
# minimum and maximum resources.
compute_config:
# compute_config:
#   head_node:
#     instance_type: m5.xlarge
#   worker_nodes:
#     - instance_type: g6.12xlarge  # 4x L4 GPUs per node
#       min_nodes: 2
#       max_nodes: 2

working_dir: .

env_vars:
  RAY_TRAIN_V2_ENABLED: "1"
  CUDA_DEVICE_MAX_CONNECTIONS: "1"  # Required for sequence parallelism
  MEGATRON_BRIDGE_ROOT: "/app/Megatron-Bridge"
  PYTHONPATH: "/app/Megatron-Bridge/src:/app/Megatron-Bridge/3rdparty/Megatron-LM"
  NCCL_DEBUG: "WARN"
  PYTHONUNBUFFERED: "1"

# Simplified entrypoint - TP=2, PP=2, batch sizes are hardcoded in script
entrypoint: >-
  python llm_sft_ray_train_megatron.py
  --hf_model_path Qwen/Qwen2.5-0.5B
  --num_workers 8
  --train_iters 100
  --storage_path /mnt/cluster_storage/megatron_experiment