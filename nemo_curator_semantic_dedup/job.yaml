# NeMo Curator Image Semantic Deduplication Job
# View the docs: https://docs.anyscale.com/reference/job-api#jobconfig

name: nemo-curator-image-dedup

# Build custom image with NeMo Curator CUDA dependencies
containerfile: ./Dockerfile

# Compute configuration with L40S GPU for CUDA-accelerated image processing
compute_config:
  head_node:
    instance_type: g6.8xlarge  # 1x L40S GPU, 32 vCPUs, 128GB RAM (AWS)
    # For GCP, use: g2-standard-32 (1x L4 GPU)
  worker_nodes: []  # Run entirely on head node

# Working directory - upload only the example code, not data
working_dir: .

# Environment variables for job configuration
# Override these when submitting to use your own data paths
env_vars:
  # Input parquet file with image URLs (TEXT and URL columns)
  # This file is copied into the Docker image during build
  INPUT_PARQUET: "/home/ray/data/truncated_100k_mscoco.parquet"
  
  # Directory for WebDataset tar files (created from parquet)
  # Use /mnt/cluster_storage for persistence, or /home/ray/data for ephemeral
  INPUT_WDS_DIR: "/mnt/cluster_storage/nemo_curator/webdataset"
  
  # Output directory for deduplicated images
  OUTPUT_DIR: "/mnt/cluster_storage/nemo_curator/results"
  
  # Directory to store CLIP embeddings
  EMBEDDINGS_DIR: "/mnt/cluster_storage/nemo_curator/embeddings"
  
  # Directory for duplicate removal parquets
  REMOVAL_DIR: "/mnt/cluster_storage/nemo_curator/removal_ids"
  
  # Model weights directory (pre-downloaded in Docker image)
  MODEL_DIR: "/home/ray/model_weights"
  
  # Processing settings
  BATCH_SIZE: "32"
  EMBEDDING_BATCH_SIZE: "32"
  TAR_FILES_PER_PARTITION: "10"
  DOWNLOAD_PROCESSES: "8"
  ENTRIES_PER_TAR: "1000"
  
  # Set to "true" to skip downloading (use existing WebDataset)
  # WebDataset already exists from previous run
  SKIP_DOWNLOAD: "false"
  
  # Ray memory settings to avoid OOM
  RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION: "0.5"
  
  # Increase Ray API server limit for cosmos_xenna monitoring
  RAY_MAX_LIMIT_FROM_API_SERVER: "100000"

# The entrypoint script
entrypoint: python image_dedup_example.py

# Don't retry on failure - easier to debug
max_retries: 0

# Kill after 4 hours to control costs (adjust based on dataset size)
timeout_s: 14400

