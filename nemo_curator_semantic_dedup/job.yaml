# NeMo Curator Image Semantic Deduplication Job
# View the docs: https://docs.anyscale.com/reference/job-api#jobconfig

name: nemo-curator-image-dedup

# Build custom image with NeMo Curator CUDA dependencies
containerfile: ./Dockerfile

# Compute configuration with L4 GPU for CUDA-accelerated image processing
# CPU-only head node + GPU worker nodes (using ignore_head_node=True in executors)
compute_config:
  head_node:
    instance_type: m6i.2xlarge  # CPU-only, 8 vCPUs, 32GB RAM
    # No tasks scheduled here - using RayDataExecutor/RayActorPoolExecutor with ignore_head_node=True
    resources:
      CPU: 0  # Prevent any task scheduling on head node
  worker_nodes:
    - instance_type: g5.12xlarge  # 4x A10G GPUs per worker, 48 vCPUs, 192GB RAM
      min_nodes: 2
      max_nodes: 2

# Working directory - use the repo root (absolute) so Curator/ is included
working_dir: /home/ray/default

# Environment variables for job configuration
# Override these when submitting to use your own data paths
env_vars:
  # Input parquet file with image URLs (TEXT and URL columns)
  # This file is copied into the Docker image during build
  INPUT_PARQUET: "/home/ray/data/truncated_100k_mscoco.parquet"
  
  # Directory for WebDataset tar files (created from parquet)
  # Use /mnt/cluster_storage for persistence, or /home/ray/data for ephemeral
  INPUT_WDS_DIR: "/mnt/cluster_storage/nemo_curator/webdataset"
  
  # Output directory for deduplicated images
  OUTPUT_DIR: "/mnt/cluster_storage/nemo_curator/results"
  
  # Directory to store CLIP embeddings
  EMBEDDINGS_DIR: "/mnt/cluster_storage/nemo_curator/embeddings"
  
  # Directory for duplicate removal parquets
  REMOVAL_DIR: "/mnt/cluster_storage/nemo_curator/removal_ids"
  
  # Model weights directory (pre-downloaded in Docker image)
  MODEL_DIR: "/home/ray/model_weights"
  
  # Processing settings (reduced to prevent OOM)
  BATCH_SIZE: "4"
  EMBEDDING_BATCH_SIZE: "8"
  TAR_FILES_PER_PARTITION: "1"
  DOWNLOAD_PROCESSES: "8"
  ENTRIES_PER_TAR: "500"
  
  # GPU allocation per worker - 2.0 = one worker per 2 GPUs (very strict limit)
  # This prevents OOM even with Ray Data pipelining overlap between stages
  EMBEDDING_GPUS_PER_WORKER: "2.0"
  

  SKIP_DOWNLOAD: "false" # Always keep false
  
  # Don't hide GPUs from tasks that request num_gpus=0 (needed for DALI)
  RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO: "0"
  
  # NCCL settings for multi-GPU K-means across nodes
  # Enable debug info to diagnose NCCL communication failures
  NCCL_DEBUG: "INFO"
  # Use the network interface that can reach other nodes
  NCCL_SOCKET_IFNAME: "ens,eth"
  # Increase timeout for initialization
  NCCL_TIMEOUT: "600"
  # Force socket-based communication (more compatible than IB)
  NCCL_NET: "Socket"
  
  # Ray memory settings to avoid OOM
  RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION: "0.3"
  # Spill objects to disk when memory is low instead of crashing
  RAY_OBJECT_SPILLING_CONFIG: '{"type":"filesystem","params":{"directory_path":"/tmp/ray_spill"}}'
  # Kill tasks that use too much memory before they OOM the node
  RAY_memory_monitor_refresh_ms: "100"
  # Force garbage collection more frequently
  RAY_ENABLE_RECORD_ACTOR_TASK_LOGGING: "1"
  
  # Increase Ray API server limit for cosmos_xenna monitoring
  RAY_MAX_LIMIT_FROM_API_SERVER: "100000"

# The entrypoint script
entrypoint: python examples/nemo_curator_semantic_dedup/image_dedup_example.py

# Don't retry on failure - easier to debug
max_retries: 0

# Kill after 4 hours to control costs (adjust based on dataset size)
timeout_s: 14400

