# View the docs https://docs.anyscale.com/reference/service-api#serviceconfig.

name: deploy-structured-output

# When empty, use the default image. This can be an Anyscale-provided base image
# like anyscale/ray:2.43.0-slim-py312-cu125, a user-provided base image (provided
# that it meets certain specs), or you can build new images using the Anyscale
# image builder at https://console.anyscale-staging.com/v2/container-images.

containerfile: ./Dockerfile

# When empty, Anyscale will auto-select the instance types. You can also specify
# minimum and maximum resources.
compute_config:
#   head_node:
#     instance_type: m5.2xlarge
#   worker_nodes:
#     - instance_type: m5.16xlarge
#       min_nodes: 0
#       max_nodes: 100
#     - instance_type: m7a.24xlarge
#       min_nodes: 0
#       max_nodes: 100
#       market_type: PREFER_SPOT # Defaults to ON_DEMAND
#     - instance_type: g4dn.2xlarge
#       min_nodes: 0
#       max_nodes: 100
#       market_type: PREFER_SPOT # Defaults to ON_DEMAND
  auto_select_worker_config: true

# Path to a local directory or a remote URI to a .zip file (S3, GS, HTTP) that
# will be the working directory for the job. The files in the directory will be
# automatically uploaded to the job environment in Anyscale.
working_dir: .

# When empty, this uses the default Anyscale Cloud in your organization.
cloud:

# Specify the Ray Serve app to deploy.
applications:
- name: structured-output-app
  route_prefix: "/"
  import_path: ray.serve.llm:build_openai_app
  args:
    llm_configs:
      - model_loading_config:
          model_id: my-qwen
          model_source: Qwen/Qwen2.5-3B-Instruct
        accelerator_type: L4
        engine_kwargs:
          max_model_len: 8192
