# View the docs https://docs.anyscale.com/reference/job-api#jobconfig.

name: skyrl-train

# When empty, use the default image. This can be an Anyscale-provided base image
# like anyscale/ray:2.50.0-slim-py312-cu128, a user-provided base image (provided
# that it meets certain specs), or you can build new images using the Anyscale
# image builder at https://console.anyscale-staging.com/v2/container-images.
# image_uri: anyscale/ray:2.50.0-slim-py312-cu128
containerfile: ./Dockerfile

# When empty, Anyscale will auto-select the instance types. You can also specify
# minimum and maximum resources.
compute_config:
#   head_node:
#     instance_type: m5.2xlarge
#   worker_nodes:
#     # These instances are only available in AWS.
#     - instance_type: p4d.24xlarge
#       min_nodes: 0
#       max_nodes: 1
#       market_type: PREFER_SPOT # Defaults to ON_DEMAND
#     - instance_type: p4de.24xlarge
#       min_nodes: 0
#       max_nodes: 1
#       market_type: PREFER_SPOT # Defaults to ON_DEMAND
#     - instance_type: p5.48xlarge
#       min_nodes: 0
#       max_nodes: 1
#       market_type: PREFER_SPOT # Defaults to ON_DEMAND
#
#     # These instances are only available in GCP.
#     - instance_type: a2-highgpu-8g-nvidia-a100-40gb-8
#       market_type: PREFER_SPOT
#     - instance_type: a2-ultragpu-8g-nvidia-a100-80gb-8
#       market_type: PREFER_SPOT
#     - instance_type: a2-megagpu-16g-nvidia-a100-40gb-16
#       market_type: PREFER_SPOT
#     - instance_type: a3-highgpu-8g-nvidia-h100-80gb-8
#       market_type: PREFER_SPOT
#     - instance_type: a3-megagpu-8g-nvidia-h100-mega-80gb-8
#       market_type: PREFER_SPOT
#     - instance_type: a3-ultragpu-8g-nvidia-h200-141gb-8
#       market_type: PREFER_SPOT
  auto_select_worker_config: true

# Path to a local directory or a remote URI to a .zip file (S3, GS, HTTP) that
# will be the working directory for the job. The files in the directory will be
# automatically uploaded to the job environment in Anyscale. This is commented out
# here because if it present, uv will look for the pyproject.toml file in that
# working directory and won't find it (instead of in the correct directory
# $HOME/SkyRL/skyrl-train).
# working_dir: .

# When empty, this uses the default Anyscale Cloud in your organization.
cloud:

env_vars:
  DATA_DIR: "/mnt/cluster_storage/data/gsm8k"
  # SkyRL has some aggressive timeouts, so we need to increase them.
  SKYRL_RAY_PG_TIMEOUT_IN_S: "600"
  # When using uv, Ray workers take a while to start up.
  RAY_worker_register_timeout_seconds: "600"

# Fetch the data and run the training.
entrypoint: |
  cd $HOME/SkyRL/skyrl-train && \
  uv run --isolated examples/gsm8k/gsm8k_dataset.py --output_dir $DATA_DIR && \
  uv run --isolated \
    --extra vllm \
    --with ray@http://localhost:9478/ray/ray-2.48.0-cp312-cp312-manylinux2014_x86_64.whl \
    -m skyrl_train.entrypoints.main_base \
    data.train_data="['$DATA_DIR/train.parquet']" \
    data.val_data="['$DATA_DIR/validation.parquet']" \
    trainer.algorithm.advantage_estimator="grpo" \
    trainer.policy.model.path="Qwen/Qwen2.5-1.5B-Instruct" \
    trainer.strategy=fsdp2 \
    trainer.placement.colocate_all=true \
    trainer.placement.policy_num_gpus_per_node=4 \
    trainer.eval_batch_size=1024 \
    trainer.eval_before_train=true \
    trainer.eval_interval=5 \
    trainer.ckpt_interval=10 \
    generator.backend=vllm \
    generator.num_inference_engines=4 \
    generator.inference_engine_tensor_parallel_size=1 \
    generator.weight_sync_backend=nccl \
    environment.env_class=gsm8k \
    trainer.logger="console" \
    trainer.project_name="gsm8k" \
    trainer.run_name="gsm8k_test"

# If there is an error, do not retry.
max_retries: 0
